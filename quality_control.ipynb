{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.io import wavfile\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quality_control import get_FC_model, get_rf_cnn_model, get_rf_LSTM_model, \\\n",
    "                            get_rf_transformer_model, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "x_all = np.load('./data/x_all.npy') # RF\n",
    "y_all = np.load('./data/y_all.npy') # QC Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split rate 0.2\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, shuffle=True, stratify=y_all)\n",
    "\n",
    "x_train = x_train.reshape([x_train.shape[0], 600, 1]).astype(np.float32)\n",
    "y_train = y_train.reshape([y_train.shape[0], 1]).astype(np.float32)\n",
    "\n",
    "x_test = x_test.reshape([x_test.shape[0], 600, 1]).astype(np.float32)\n",
    "y_test = y_test.reshape([y_test.shape[0], 1]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced sampling\n",
    "pos_ids = np.where(y_train==1)\n",
    "x_train_pos = x_train[pos_ids[0],:,:]\n",
    "y_train_pos = np.zeros([x_train_pos.shape[0],2])\n",
    "y_train_pos[:,0] = 1\n",
    "\n",
    "neg_ids = np.where(y_train==0)\n",
    "x_train_neg = x_train[neg_ids[0],:,:]\n",
    "y_train_neg = np.zeros([x_train_neg.shape[0],2])\n",
    "y_train_neg[:,1] = 1\n",
    "\n",
    "pos_dataset = tf.data.Dataset.from_tensor_slices((x_train_pos, y_train_pos)).repeat()\n",
    "neg_dataset = tf.data.Dataset.from_tensor_slices((x_train_neg, y_train_neg)).repeat()\n",
    "balanced_dataset = tf.data.experimental.sample_from_datasets([pos_dataset, neg_dataset],weights=[0.5, 0.5])\n",
    "\n",
    "# reshape y_test\n",
    "pos_ids = np.where(y_test==1)\n",
    "x_test_pos = x_test[pos_ids[0],:,:]\n",
    "y_test_pos = np.zeros([x_test_pos.shape[0],2])\n",
    "y_test_pos[:,0] = 1\n",
    "\n",
    "neg_ids = np.where(y_test==0)\n",
    "x_test_neg = x_test[neg_ids[0],:,:]\n",
    "y_test_neg = np.zeros([x_test_neg.shape[0],2])\n",
    "y_test_neg[:,1] = 1\n",
    "\n",
    "pos_dataset_test = tf.data.Dataset.from_tensor_slices((x_test_pos, y_test_pos)).repeat()\n",
    "neg_dataset_test = tf.data.Dataset.from_tensor_slices((x_test_neg, y_test_neg)).repeat()\n",
    "balanced_dataset_test = tf.data.experimental.sample_from_datasets([pos_dataset_test, neg_dataset_test],weights=[0.5, 0.5])\n",
    "\n",
    "# reshape y_test\n",
    "pos_val_ids = np.where(y_test==1)\n",
    "neg_val_ids = np.where(y_test==0)\n",
    "y_test_temp = np.zeros([y_test.shape[0], 2])\n",
    "y_test_temp[pos_val_ids[0], 0] = 1\n",
    "y_test_temp[neg_val_ids[0], 1] = 1\n",
    "y_test = y_test_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully-conneted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "input_shape = [x_train.shape[1], 1]\n",
    "model = get_FC_model(input_shape)\n",
    "checkpoint = ModelCheckpoint(\"FC_model_{epoch:d}_{val_loss:.4f}.h5\", monitor='val_loss', period=1, save_best_only=True, mode='min')\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', mode='min',patience=20, min_delta=0.003)\n",
    "history = model.fit(balanced_dataset.repeat().shuffle(1000).batch(8), steps_per_epoch=200, epochs=2000, verbose=True, callbacks=[checkpoint, earlyStopping], validation_data=balanced_dataset_test.shuffle(100).batch(8),validation_steps=200)\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras_auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "accuracy = history.history['loss']\n",
    "val_accuracy = history.history['val_loss']\n",
    "epochs = range(1, len(accuracy)+1)\n",
    "\n",
    "plt.plot(epochs, accuracy, label='Training loss')\n",
    "plt.plot(epochs, val_accuracy, label='Validation loss')\n",
    "plt.xlabel('Epoch', size=20)\n",
    "plt.ylabel('Loss', size=20)\n",
    "plt.ylim([0,1.0])\n",
    "plt.xlim([0,len(accuracy)+1])\n",
    "plt.yticks(size=20)\n",
    "plt.xticks(size=20)\n",
    "plt.legend()\n",
    "plt.savefig(\"history_FC\", dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, fontsize=12)\n",
    "    plt.yticks(tick_marks, classes, fontsize=12)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 fontsize=14,\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=12)\n",
    "    plt.xlabel('Predicted label', fontsize=12)\n",
    "    plt.savefig(title, dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "input_shape = [x_train.shape[1], 1]\n",
    "model = get_FC_model(input_shape)\n",
    "model.load_weights('./model/FC_model_39_0.3987.h5')\n",
    "# Evaluate the model on the dataset\n",
    "results = model.evaluate(x_test, y_test, batch_size=8)\n",
    "print('test loss: %.6f, test acc:%.6f:', results)\n",
    "predictions = model.predict(x_test)\n",
    "true_class = tf.argmax( y_test, 1 )\n",
    "predicted_class = tf.argmax( predictions, 1 )\n",
    "confusion_matrix = tf.math.confusion_matrix( true_class, predicted_class, 2 )\n",
    "plot_confusion_matrix(confusion_matrix.numpy(), classes=['Good','Bad'], normalize=True)#, title='FC Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "input_shape = [x_train.shape[1], 1]\n",
    "model = get_rf_cnn_model(input_shape)\n",
    "checkpoint = ModelCheckpoint(\"CNN_model_{epoch:d}_{val_loss:.4f}.h5\", monitor='val_loss', period=1, save_best_only=True, mode='min')\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', mode='min', patience=50, min_delta=0.002)\n",
    "history = model.fit(balanced_dataset.repeat().shuffle(1000).batch(8), steps_per_epoch=200, epochs=3000, verbose=True, callbacks=[checkpoint, earlyStopping], validation_data=balanced_dataset_test.shuffle(100).batch(8), validation_steps=200)\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras_auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "accuracy = history.history['loss']\n",
    "val_accuracy = history.history['val_loss']\n",
    "epochs = range(1, len(accuracy)+1)\n",
    "\n",
    "plt.plot(epochs, accuracy, label='Training loss')\n",
    "plt.plot(epochs, val_accuracy, label='Validation loss')\n",
    "plt.xlabel('Epoch', size=20)\n",
    "plt.ylabel('Loss', size=20)\n",
    "plt.ylim([0,1.0])\n",
    "plt.xlim([0,len(accuracy)+1])\n",
    "plt.yticks(size=20)\n",
    "plt.xticks(size=20)\n",
    "plt.legend()\n",
    "plt.savefig(\"history_CNN\", dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "input_shape = [x_train.shape[1], 1]\n",
    "model = get_rf_cnn_model(input_shape)\n",
    "model.load_weights('./model/CNN_model_63_0.2767.h5')\n",
    "# Evaluate the model on the dataset\n",
    "results = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('test loss: %.6f, test acc:%.6f:', results)\n",
    "predictions = model.predict(x_test)\n",
    "true_class = tf.argmax( y_test, 1 )\n",
    "predicted_class = tf.argmax( predictions, 1 )\n",
    "confusion_matrix = tf.math.confusion_matrix( true_class, predicted_class, 2 )\n",
    "plot_confusion_matrix(confusion_matrix.numpy(), classes=['Good','Bad'], normalize=True, title='CNN Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "input_shape = [x_train.shape[1], 1]\n",
    "model = get_rf_LSTM_model(input_shape)\n",
    "checkpoint = ModelCheckpoint(\"LSTM_model_{epoch:d}_{val_loss:.4f}.h5\", monitor='val_loss', period=1, save_best_only=True,mode='min')\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=50, mode='min', min_delta=0.001)\n",
    "history = model.fit(balanced_dataset.repeat().shuffle(1000).batch(8), steps_per_epoch=200, epochs=5000, verbose=True, callbacks=[checkpoint, earlyStopping], validation_data=balanced_dataset_test.shuffle(100).batch(8), validation_steps=200)\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras_auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "accuracy = history.history['loss']\n",
    "val_accuracy = history.history['val_loss']\n",
    "epochs = range(1, len(accuracy)+1)\n",
    "\n",
    "plt.plot(epochs, accuracy, label='Training loss')\n",
    "plt.plot(epochs, val_accuracy, label='Validation loss')\n",
    "plt.xlabel('Epoch',size=20)\n",
    "plt.ylabel('Loss',size=20)\n",
    "plt.ylim([0,1.0])\n",
    "plt.xlim([0,len(accuracy)+1])\n",
    "plt.yticks(size=20)\n",
    "plt.xticks(size=20)\n",
    "plt.legend(prop={'size':20})\n",
    "plt.savefig(\"history_LSTM\", dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "input_shape = [x_train.shape[1], 1]\n",
    "model = get_rf_LSTM_model(input_shape)\n",
    "model.load_weights('./model/LSTM_model_95_0.2452.h5')\n",
    "# Evaluate the model on the dataset\n",
    "results = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('test loss: %.6f, test acc:%.6f:', results)\n",
    "predictions = model.predict(x_test)\n",
    "true_class = tf.argmax( y_test, 1 )\n",
    "predicted_class = tf.argmax( predictions, 1 )\n",
    "confusion_matrix = tf.math.confusion_matrix( true_class, predicted_class, 2 )\n",
    "plot_confusion_matrix(confusion_matrix.numpy(), classes=['Good','Bad'],normalize=True, title='LSTM Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-BiLSTM-Trans model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "input_shape = [x_train.shape[1], 1]\n",
    "model = get_rf_transformer_model(input_shape)\n",
    "checkpoint = ModelCheckpoint(\"Transformer_model_{epoch:d}_{val_loss:.4f}.h5\", monitor='val_loss', period=1, save_best_only=True,mode='min')\n",
    "earlyStopping = EarlyStopping(monitor='val_loss',mode='min', patience=50, min_delta=0.003)\n",
    "history = model.fit(balanced_dataset.repeat().shuffle(1000).batch(8), steps_per_epoch=200, epochs=5000, verbose=True, callbacks=[checkpoint, earlyStopping], validation_data=balanced_dataset_test.shuffle(100).batch(8),validation_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "accuracy = history.history['loss']\n",
    "val_accuracy = history.history['val_loss']\n",
    "epochs = range(1, len(accuracy)+1)\n",
    "\n",
    "plt.plot(epochs, accuracy, label='Training loss')\n",
    "plt.plot(epochs, val_accuracy, label='Validation loss')\n",
    "plt.xlabel('Epoch',size=20)\n",
    "plt.ylabel('Loss',size=20)\n",
    "plt.ylim([0,1.0])\n",
    "plt.xlim([0,len(accuracy)+1])\n",
    "plt.yticks(size=20)\n",
    "plt.xticks(size=20)\n",
    "plt.legend(prop={'size':20})\n",
    "plt.savefig(\"history_Transformer\", dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "input_shape = [x_train.shape[1], 1]\n",
    "model = get_rf_transformer_model(input_shape)\n",
    "model.load_weights('./model/Transformer_model_35_0.2734.h5')\n",
    "# Evaluate the model on the dataset\n",
    "results = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('test loss: %.6f, test acc:%.6f:', results)\n",
    "predictions = model.predict(x_test)\n",
    "true_class = tf.argmax( y_test, 1 )\n",
    "predicted_class = tf.argmax( predictions, 1 )\n",
    "confusion_matrix = tf.math.confusion_matrix( true_class, predicted_class, 2 )\n",
    "plot_confusion_matrix(confusion_matrix.numpy(), classes=['Good','Bad'], normalize=True, title='Transformer Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on RFs from smaller earthquake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_tmp = np.load('./data/small_earthquake_RF.npy') # 5.0 < mag < 5.5\n",
    "\n",
    "input_shape = [choose_tmp.shape[1], 1]\n",
    "model = get_rf_LSTM_model(input_shape)\n",
    "model.load_weights('./model/LSTM_model_95_0.2452.h5')\n",
    "predictions = model.predict(choose_tmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
